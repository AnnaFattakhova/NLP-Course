{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aIaBWnAtrXUj",
        "xSzWW0WL1KwO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Домашнее задание:\n",
        "\n",
        "1) Повторить проведенный эксперимент на любом другом корпусе на выбор из туториала https://www.cis.lmu.de/~fraser/EMA2008/model1.html (по желанию)\n",
        "\n",
        "2) Своими словами описать принцип работы SMT (основные компоненты, важную терминологию) - сделайте это в формате конспекта-шпаргалки, к которой вы сможете вернуться в будущем (обязательно)"
      ],
      "metadata": {
        "id": "dAbYrqwlYpd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Формальное определение SMT** (SMT = Statistical machine translation, статистический машинный перевод; перевод генерируется за счет статистических методов, параметры для которых появляются из анализа двуязычных корпусов)\n",
        "\n",
        "Есть предложение на языке `L1`, задача -- найти наиболее правдоподобный перевод на язык `L2`. Это делается при помощи математических методов. Необходимо найти предложение, которое максимизирует `P(L2|L1)`(`argmax P(L2|L1)`), для этого вычисяем:\n",
        "1. допустимые предложения `L2` путем создания языковой модели `P(L2)`;\n",
        "2. допустимые пары `L1 - L2` (сопоставляем статистические совпадения выравненных (=оригинал совпадает с переводом) фраз в параллельном корпусе `P(L1|L2)`);\n",
        "\n",
        "И максимизируем результат вычисления `P(L2) P(L1|L2)`(`argmax P(L2) P(L1|L2)`)\n",
        "\n",
        "**Компоненты модели SMT**\n",
        "\n",
        "1. **N-граммная языковая модель** (вероятностное распределение конструкций слов или фраз в L2)\n",
        "2. **Модель перевода (t-model)** (статистика переводческих соотвествий `L1 - L2` в паралелльном корпусе за счет поиска наиболее правдоподобных соответствий `L1 - L2` среди всех предложений `L2`)\n",
        "3. **Декодер** (выбор наиболее грамматичного и лексически правдоподобного результата среди гипотез)"
      ],
      "metadata": {
        "id": "_r_HX-Hzj5bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Работа программы:**\n",
        "\n",
        "1) Загрузка библиотек ([`sklearn`](https://scikit-learn.org/stable/))"
      ],
      "metadata": {
        "id": "Lqxu42Gjzz9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import random"
      ],
      "metadata": {
        "id": "Z1RYdkLYzwYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Препроцессинг\n",
        "\n",
        "*- параллельные корпуса* ([OPUS](https://opus.nlpl.eu/) Corpora, [Kaggle](https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench), [HuggingFace](https://huggingface.co/datasets?task_categories=task_categories:translation&sort=trending))"
      ],
      "metadata": {
        "id": "jSJwY2yt0NeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# извлекаем файлы из архива\n",
        "with tarfile.open('toy.tgz', 'r:gz') as tar:\n",
        "  tar.extractall()\n",
        "\n",
        "# просматриваем, что распаковали\n",
        "!ls\n",
        "\n",
        "# создаем 2 выборки, токенизируем по фразам\n",
        "with open('toy.de', 'r') as f:\n",
        "  german = f.read().split('\\n')[:-1]\n",
        "\n",
        "with open('toy.en', 'r') as f:\n",
        "  english = f.read().split('\\n')[:-1]\n",
        "\n",
        "print(\"Данные языка X:\\n\", german)\n",
        "print(\"Данные языка Y:\\n\", english)\n",
        "\n",
        "# при помощи sklearn делим выборку\n",
        "X_train, X_test, y_train, y_test = train_test_split(english, german)\n",
        "\n",
        "print(\"> Обучающая выборка:\")\n",
        "for text, label in zip(X_train, y_train):\n",
        "    print(f\"\\nТекст на немецком: {label}\\n Его перевод на английский: {text}\\n\")\n",
        "\n",
        "print(\"> Тестовая выборка:\")\n",
        "for text, label in zip(X_test, y_test):\n",
        "    print(f\"\\nТекст на немецком: {label}\\n Его перевод на английский: {text}\\n\")"
      ],
      "metadata": {
        "id": "NFxOS4Zv1YBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*- подготовка данных* (на этом этаппе можно почистить данные, выделить n-граммы; далее токенизацируем каждую фразу по словам)"
      ],
      "metadata": {
        "id": "3L3t_B4x2qS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "  # функция возвращает списки слов\n",
        "  return [sentence.split() for sentence in sentences]\n",
        "\n",
        "# токенизируем каждую выборку\n",
        "X_train_tokens, X_test_tokens, y_train_tokens, y_test_tokens = tokenize(X_train), tokenize(X_test), tokenize(y_train), tokenize(y_test)\n",
        "\n",
        "print('Образец токенизированного текста:', X_train_tokens)\n",
        "\n",
        "# создаем словарь уникальных словоформ\n",
        "x_vocab = Counter(' '.join(german).split()).keys()\n",
        "y_vocab = Counter(' '.join(english).split()).keys()\n",
        "\n",
        "print(f\"Словарь немецких словоформ: {x_vocab}\\n Всего {len(x_vocab)} словоформ\")\n",
        "print(f\"\\nCловарь английских словоформ: {y_vocab}\\n Всего {len(y_vocab)} словоформ\")"
      ],
      "metadata": {
        "id": "GhORQ_oU2rL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Модель SMT:\n",
        "\n",
        " *- IBM 1 Expectation-Maximization (t-model)*"
      ],
      "metadata": {
        "id": "N8VL04-906AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# вероятность того, что случайное слово x_vocab соответсвует случайному слову y_vocab\n",
        "uniform = 1 / (len(x_vocab) * len(y_vocab))\n",
        "\n",
        "round(uniform, 3)\n",
        "\n",
        "# t-model\n",
        "t = {}\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  # начинаем итерацию по обучающей выборке\n",
        "  for word_x in X_train_tokens[i]:\n",
        "    for word_y in y_train_tokens[i]:\n",
        "      # создаем t-table\n",
        "      t[(word_x, word_y)] = uniform\n",
        "\n",
        "# t-table\n",
        "for elem in t:\n",
        "  print(\"Соответствие |\", elem[0], \"  ->  \", elem[1], \"| Вероятность:\", round(t[elem], 3))\n",
        "\n",
        "# количество итераций обучения\n",
        "epochs = 7\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # начинаем обучение\n",
        "\n",
        "  # шаг 0. создаем слоты для подсчета статистики\n",
        "  count = {} # P(x|y)\n",
        "  total = {} # P(y)\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    # начинаем итерацию по обучающей выборке\n",
        "    for word_x in X_train_tokens[i]:\n",
        "      for word_y in y_train_tokens[i]:\n",
        "        # создаем слоты для подсчета условной вероятности совпадений в корпусе\n",
        "        count[(word_x, word_y)] = 0\n",
        "        # и слоты для статистической языковой модели y\n",
        "        total[word_y] = 0\n",
        "\n",
        "  # шаг 1. Expectation\n",
        "  for i in range(len(X_train)):\n",
        "    # начинаем итерацию по обучающей выборке\n",
        "    total_stat = {} # статистика x\n",
        "\n",
        "    # собираем предварительную статистику на основе данных x\n",
        "    for word_x in X_train_tokens[i]:\n",
        "      total_stat[word_x] = 0 # создаем слоты для подсчета статистики по каждому токену x\n",
        "      for word_y in y_train_tokens[i]:\n",
        "        # обновляем данные из t-table; увеличиваем значения при обнаружении совместной встречаемости\n",
        "        total_stat[word_x] += t[(word_x, word_y)]\n",
        "\n",
        "    # обновляем данные для P(x|y) и P(y)\n",
        "    for word_x in X_train_tokens[i]:\n",
        "      for word_y in y_train_tokens[i]:\n",
        "        # подсчет условной вероятности совпадений в корпусе: равномерное распределение / частотность x\n",
        "        count[(word_x, word_y)] += t[(word_x, word_y)] / total_stat[word_x]\n",
        "        # подсчет статистической информации y: равномерное распределение / частотность x\n",
        "        total[word_y] += t[(word_x, word_y)] / total_stat[word_x]\n",
        "\n",
        "  # шаг 2. Maximization\n",
        "  for i in range(len(X_train)):\n",
        "    # начинаем итерацию по обучающей выборке\n",
        "    for word_x in X_train_tokens[i]:\n",
        "      for word_y in y_train_tokens[i]:\n",
        "        # обновляем t-table: вероятность совпадения в корпусе / вероятность информации y\n",
        "        t[(word_x, word_y)] = count[(word_x, word_y)] / total[word_y]\n",
        "\n",
        "for elem in t:\n",
        "  print(\"Соответствие |\", elem[0], \"  ->  \", elem[1], \"| Вероятность:\", round(t[elem], 3))\n"
      ],
      "metadata": {
        "id": "LHvxD-WD33uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*- Биграммная модель*"
      ],
      "metadata": {
        "id": "h5dFN8JI34GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# для обучения модели объединим 2 выборки\n",
        "tokens = ' '.join(german).split()\n",
        "\n",
        "# хранилище для биграмм\n",
        "bigram_model = defaultdict(list)\n",
        "\n",
        "# собираем все попарные совпадения\n",
        "for i in range(len(tokens)-1):\n",
        "    current_word = tokens[i]\n",
        "    next_word = tokens[i + 1]\n",
        "    bigram_model[current_word].append(next_word)\n",
        "\n",
        "print(bigram_model)\n",
        "\n",
        "def decoder(model, steps=5):\n",
        "  # инициализация случайного токена\n",
        "  current_word = random.choice(tokens)\n",
        "  generated_sentence = current_word\n",
        "\n",
        "  for step in range(steps):\n",
        "    # пошаговая генерация\n",
        "    print('Шаг', step+1)\n",
        "    next_word_options = model[current_word]\n",
        "    print(f'Правдоподобные варианты продолжения для токена {current_word}:', next_word_options)\n",
        "\n",
        "    current_word = random.choice(next_word_options)\n",
        "    generated_sentence += ' '\n",
        "    generated_sentence += current_word\n",
        "    print('Промежуточный результат:', generated_sentence)\n",
        "    print()\n",
        "  print('Результат:', generated_sentence)\n",
        "\n",
        "decoder(bigram_model)"
      ],
      "metadata": {
        "id": "PJtOlF3u5SyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Оценка результатов ([NLTK Translate](https://www.nltk.org/api/nltk.translate.bleu_score.html))"
      ],
      "metadata": {
        "id": "PXZcckbh07qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# сортировка t-table по убыванию правдоподобия\n",
        "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
        "\n",
        "def translate(token):\n",
        "  for element in sorted_t:\n",
        "    if element[0][1] == token:\n",
        "      # поиск совпадений в t-table\n",
        "      return element[0][0]\n",
        "\n",
        "for sentence in y_test_tokens:\n",
        "  print(\"Оригинальное предложение:\", ' '.join(sentence))\n",
        "  translation = []\n",
        "  for token in sentence:\n",
        "    translation.append(translate(token))\n",
        "  print(\"Перевод:\", ' '.join(translation))\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "reference = [X_test_tokens[0], X_test_tokens[1]]\n",
        "candidate = [translate(token) for token in y_test_tokens[0]]\n",
        "\n",
        "bleu_score = corpus_bleu(reference, candidate)\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n",
        "\n",
        "# --> reference\n",
        "# --> candidate\n",
        "\n"
      ],
      "metadata": {
        "id": "z5v2c8pL4Yc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}